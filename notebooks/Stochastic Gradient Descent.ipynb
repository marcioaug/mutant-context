{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load notebooks/utils/get_dataframes.py\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "MUTANTS_CONTEXT = '{0}/{1}/mutants.context'\n",
    "KILL_CSV = '{0}/{1}/kill.csv'\n",
    "\n",
    "\n",
    "def get_subject(subject):\n",
    "\n",
    "    mutants_context_df = pd.read_csv(MUTANTS_CONTEXT.format(DATA_DIR, subject))\n",
    "    kill_df = pd.read_csv(KILL_CSV.format(DATA_DIR, subject))\n",
    "\n",
    "    del mutants_context_df['mutationOperatorGroup']\n",
    "    del mutants_context_df['nodeTypeDetailed']\n",
    "    del mutants_context_df['nodeContextBasic']\n",
    "    del mutants_context_df['astContextDetailed']\n",
    "    del mutants_context_df['parentContextBasic']\n",
    "    del mutants_context_df['parentContextDetailed']\n",
    "    del mutants_context_df['parentStmtContextBasic']\n",
    "    del mutants_context_df['parentStmtContextDetailed']\n",
    "\n",
    "    kill = []\n",
    "\n",
    "    for i in mutants_context_df['mutantNo']:\n",
    "\n",
    "        search = kill_df.query('MutantNo == %d' % i)\n",
    "\n",
    "        if len(search) > 0:\n",
    "            kill.append(search.iloc[0, 1])\n",
    "        else:\n",
    "            kill.append(None)\n",
    "\n",
    "    mutants_context_df['kill'] = kill\n",
    "\n",
    "    return mutants_context_df.dropna(axis=0, how='any').reset_index(drop=True)\n",
    "\n",
    "\n",
    "def get_subjects(subjects):\n",
    "    mutant_context_df = None\n",
    "\n",
    "    for subject in subjects:\n",
    "        if mutant_context_df is not None:\n",
    "            mutant_context_df = mutant_context_df.append(\n",
    "                get_subject(subject))\n",
    "        else:\n",
    "            mutant_context_df = get_subject(subject)\n",
    "\n",
    "    return mutant_context_df.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %load notebooks/utils/get_x.py\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def get_x(df):\n",
    "    vectorizer_ast = CountVectorizer(ngram_range=(1, 4), analyzer='word')\n",
    "    ast_vectorized = vectorizer_ast.fit_transform(\n",
    "        df['astContextBasic'])\n",
    "\n",
    "    vectorizer_mutation_op = CountVectorizer()\n",
    "    mutation_op_vectorized = vectorizer_mutation_op.fit_transform(\n",
    "        df['mutationOperator'])\n",
    "\n",
    "    vectorizer_node_type = CountVectorizer()\n",
    "    node_type_vectorized = vectorizer_node_type.fit_transform(\n",
    "        df['nodeTypeBasic'])\n",
    "\n",
    "    ast_df = pd.DataFrame(\n",
    "        ast_vectorized.A,\n",
    "        columns=vectorizer_ast.get_feature_names())\n",
    "\n",
    "    mutation_op_df = pd.DataFrame(\n",
    "        mutation_op_vectorized.A,\n",
    "        columns=vectorizer_mutation_op.get_feature_names())\n",
    "\n",
    "    node_type_df = pd.DataFrame(\n",
    "        node_type_vectorized.A,\n",
    "        columns=vectorizer_node_type.get_feature_names())\n",
    "\n",
    "    x = df.iloc[:, 4:7]\n",
    "    x = x.join(ast_df.add_prefix('ast_'))\n",
    "    x = x.join(mutation_op_df.add_prefix('mop_'))\n",
    "    x = x.join(node_type_df.add_prefix('nty_'))\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load notebooks/utils/get_y.py\n",
    "def get_y(df, column):\n",
    "    status_to_nonequivalent = {\n",
    "        'FAIL': 'NON_EQUIVALENT',\n",
    "        'TIME': 'NON_EQUIVALENT',\n",
    "        'EXC': 'NON_EQUIVALENT',\n",
    "        'LIVE': 'MAYBE_EQUIVALENT'\n",
    "    }\n",
    "\n",
    "    status_to_trivial = {\n",
    "        'FAIL': 'NON_TRIVIAL',\n",
    "        'TIME': 'NON_TRIVIAL',\n",
    "        'EXC': 'TRIVIAL',\n",
    "        'LIVE': 'NON_TRIVIAL'\n",
    "    }\n",
    "\n",
    "    df['non_equivalent'] = df['kill'].str.upper().map(status_to_nonequivalent)\n",
    "    df['trivial'] = df['kill'].str.upper().map(status_to_trivial)\n",
    "\n",
    "    return df[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load notebooks/utils/metrics.py\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def measure_performance(X, y, clf):\n",
    "    y_predicted = clf.predict(X)\n",
    "    print('Accuracy: %f \\n' % metrics.accuracy_score(y, y_predicted))\n",
    "    print(metrics.classification_report(y, y_predicted), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load notebooks/utils/samples.py\n",
    "def get_samples(df, label, frac=0.1, replace=True):\n",
    "    X = get_x(df)\n",
    "    X['y'] = get_y(df, label)\n",
    "\n",
    "    X_train = X.sample(frac=frac, replace=replace)\n",
    "\n",
    "    y = X['y']\n",
    "    y_train = X_train['y']\n",
    "\n",
    "    X = X.drop(['y'], axis=1)\n",
    "    X_train = X_train.drop(['y'], axis=1)\n",
    "\n",
    "    return X, y, X_train, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.822086 \n\n                  precision    recall  f1-score   support\n\nMAYBE_EQUIVALENT       0.65      0.43      0.52        72\n  NON_EQUIVALENT       0.85      0.93      0.89       254\n\n     avg / total       0.81      0.82      0.81       326\n \n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/idp/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:84: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "label = 'non_equivalent'\n",
    "\n",
    "subjects = [\n",
    "    'Closure-5', 'Closure-8', 'Closure-12'\n",
    "]\n",
    "\n",
    "\n",
    "mutant_context_df = get_subjects(subjects)\n",
    "\n",
    "X, y, X_train, y_train = get_samples(mutant_context_df, label, frac=1)\n",
    "\n",
    "\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\")\n",
    "\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "measure_performance(X, y, clf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
